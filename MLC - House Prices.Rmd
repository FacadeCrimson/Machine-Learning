---
title: "Full Machine Learning"
author: "Lechen Tan"
date: "3/17/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library('tidyverse') 
library('scales')
library('caret')
library('mice')
library ('MASS')
library('klaR')
library('mlbench')
library('kernlab')
library('randomForest')
library('glmnet')
library('e1071')
library('rpart')
library('pls')
library('xgboost')
library('neuralnet')
library('doParallel')
```

Part 1: Data Reading and Exploratory Data Analysis
```{r}
#Read in train and test data:
training <- read.csv("/Users/apple/Desktop/MLC - House Prices/train.csv")

testing  <- read.csv('/Users/apple/Desktop/MLC - House Prices/test.csv')

#Unify factor levels
for(x in 1:81){
  if(is.factor(training[,x])){
    levels(testing[,x]) <- levels(training[,x])
  }
}

#rows
nrow(training)
nrow(testing)

train=1:1460
test=-(train)

#full dataset
full  <- bind_rows(training, testing)

str(full)
```

```{r}
#Count of NAs
sapply(full,function(x)sum(is.na(x)))
```
```{r}
#Convert to categorical variables
full$MSSubClass <- as.factor(full$MSSubClass)
full$MoSold <- as.factor(full$MoSold)
full$BsmtFullBath <- as.factor(full$BsmtFullBath)
full$BsmtHalfBath <- as.factor(full$BsmtHalfBath)
full$KitchenQual <- as.factor(full$KitchenQual)
```

```{r}
#Table of categorical variables
sapply(full,function(x)if(is.factor(x))table(x))
```

Part2: Data Preprocessing

```{r}
#Dropping due to mostly single class or mostly 0s.
full$Id <- NULL
full$Street <- NULL
full$Alley <- NULL
full$Utilities <- NULL
full$LowQualFinSF <- NULL
full$X3SsnPorch <- NULL
full$PoolArea <- NULL
full$PoolQC <- NULL
full$MiscFeature <- NULL
full$MiscVal <- NULL
full$Condition2 <- NULL
full$GarageYrBlt <- NULL
```


```{r}
#Ordinal Encoding
full$ExterQual<- as.integer(factor(full$ExterQual,levels=c("Fa","TA","Gd","Ex")))
full$ExterCond<- as.integer(factor(full$ExterCond,levels=c("Po","Fa","Gd","TA","Ex")))
full$BsmtQual<- as.integer(factor(full$BsmtQual,exclude = NULL,levels=c(NA,"Fa","TA","Gd","Ex"),labels = c(1, 2, 3, 4, 5)))
full$BsmtCond<- as.integer(factor(full$BsmtCond,exclude = NULL,levels=c("Po",NA,"Fa","TA","Gd"),labels = c(1, 2, 3, 4, 5)))
full$BsmtExposure<- as.integer(factor(full$BsmtExposure,exclude = NULL,levels=c(NA,"No","Mn","Av","Gd"),labels = c(1, 2, 3, 4, 5)))
full$BsmtFinType1<- as.integer(factor(full$BsmtFinType1,exclude = NULL,levels=c(NA,"Rec","BLQ","LwQ","ALQ","Unf","GLQ"),labels = c(1, 2, 3, 4, 5, 6, 7)))
full$BsmtFinType2<- as.integer(factor(full$BsmtFinType2,exclude = NULL,levels=c(NA,"BLQ","LwQ","Rec","GLQ","Unf","ALQ"),labels = c(1, 2, 3, 4, 5, 6, 7)))
full$HeatingQC<- as.integer(factor(full$HeatingQC,levels=c("Po","Fa","TA","Gd","Ex")))
full$FireplaceQu<- as.integer(factor(full$FireplaceQu,exclude = NULL,levels=c( "Po",NA,"Fa","TA","Gd","Ex"),labels = c(1, 2, 3, 4, 5, 6)))
full$GarageFinish<- as.integer(factor(full$GarageFinish,exclude = NULL,levels=c(NA,"Unf","RFn","Fin"),labels = c(1, 2, 3, 4)))
full$GarageQual<- as.integer(factor(full$GarageQual,exclude = NULL,levels=c(NA,"Po","Fa","TA","Gd","Ex"),labels = c(1, 2, 3, 4, 5, 6)))
full$GarageCond<- as.integer(factor(full$GarageCond,exclude = NULL,levels=c(NA,"Po","Fa","Ex","Gd","TA"),labels = c(1, 2, 3, 4, 5, 6)))
full$PavedDrive<- as.integer(factor(full$PavedDrive,levels=c("N","P","Y")))
full$Fence<- as.integer(factor(full$Fence,exclude = NULL,levels=c("MnWw","GdWo","MnPrv","GdPrv",NA),labels = c(1, 2, 3, 4, 5)))
full$GarageType <- factor(full$GarageType,exclude=NULL,levels=c(NA,"2Types","Attchd","Basment","BuiltIn","CarPort","Detchd" ),labels = c("None","2Types","Attchd","Basment","BuiltIn","CarPort","Detchd"))
```



```{r}
#Check validity of ordinal encoding by summarizing average SalePrice of each factor level, adjust order if the average is not ascending with encoding number.
#Alter the order of ExterCond, BsmtCond,BsmtFinType1,BsmtFinType2,FireplaceQu,GarageCond,Fence
summarize(group_by(full,GarageCond),
          mean(SalePrice, na.rm=T))

```




```{r}
#Imputation of missing values using key characteristics:

for(x in c('MSZoning','Exterior1st','Exterior2nd','MasVnrType','Electrical','BsmtFullBath','BsmtHalfBath','KitchenQual','Functional','SaleType','LotFrontage','MasVnrArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','GarageCars','GarageArea')){

  mice_mod <- mice(full[, names(full) %in% c(x,'MSSubClass','LotArea','Neighborhood','HouseStyle','OverallQual','YearBuilt')], m=3,method='rf') 
  mice_output <- complete(mice_mod)
  full[,x] <- mice_output[,x]
}

```
```{r}
full$KitchenQual<- as.integer(factor(full$KitchenQual,levels=c("Fa","TA","Gd","Ex")))
```


```{r}
training <- full[train,-69]
testing <- full[test,-69]
```



```{r}
#Preprocess: BoxCox transformation, centering and standardizing,pca
pred <- preProcess(training,method=c("BoxCox","center","scale","pca"))
train.t <- predict(pred,training)
test.t <- predict(pred,testing)
full.t <- bind_rows(train.t, test.t)
#full.t <- predict(pred,full[,-69])
full.t$SalePrice <- log(full$SalePrice)
pred$method



```
```{r}
#Check validity of transformation
#x <- sapply(full.t[train,],function(x)if(is.numeric(x))mean(x))
#data.frame(x[sapply(x,function(x)!is.null(x))])
```


```{r}
#Encoding all categorical variables
x <- dummyVars( ~., data = full.t[-57],fullRank=TRUE)
full.t <- data.frame(predict(x,newdata=full.t))
full.t$SalePrice <- log(full$SalePrice)
head(full.t)
```


```{r}
#Train and test set split
z <- full.t[train,]
partition <- createDataPartition(y=z$SalePrice,
                                 p=.5,
                                 list=F)
tr <- z[partition,]
te <- z[-partition,]
```



Feature Selection

Lasso: SelectFromModel

```{r}
#Automatic feature selection
set.seed(100)
cl <- makePSOCKcluster(5)
registerDoParallel(cl)

rfe.con <- rfeControl(functions=rfFuncs, method="cv", number=5)

results <- rfe(x.train[,27:56], log(x.train[,57]), sizes=c(10,13,16,19,22,25), rfeControl=rfe.con)

stopCluster(cl)
```


```{r}
print(results)

predictors(results)

plot(results, type=c("g", "o"))
```



Part3: Model Fitting



```{r,cache=TRUE}
#Linear regression
lm.fit=lm(SalePrice~.,data=tr) 
summary(lm.fit)
```


```{r,cache=TRUE}
lm.pred=predict(lm.fit,te)
RMSE(lm.pred,te$SalePrice)

```


```{r}
#Matrix transformation of tr and te:
x.trainm=data.matrix(tr)[,-205]
x.testm=data.matrix(te)[,-205]
y.trainm=unlist(tr[205])
```


```{r,cache=TRUE}
#Ridge Penalized Regression
set.seed(100)
# Find the best lambda using cross-validation
cv.ridge=cv.glmnet(x.trainm,y.trainm,alpha=0) 
# Fit the final model on the training data
ridge.fit=glmnet(x.trainm,y.trainm,alpha=0,lambda = cv.ridge$lambda.min)
coef(ridge.fit)
```

```{r}
# Check RMSE
ridge.pred=predict(ridge.fit,newx=x.testm) 
RMSE(ridge.pred,te$SalePrice)
```


```{r,cache=TRUE}
#Lasso Penalized Regression
set.seed(100)
# Find the best lambda using cross-validation
cv.lasso=cv.glmnet(x.trainm,y.trainm,alpha=1) 
# Fit the final model on the training data
lasso.fit=glmnet(x.trainm,y.trainm,alpha=1,lambda = cv.lasso$lambda.min)
coef(lasso.fit)
```

```{r}
# Check RMSE
lasso.pred=predict(lasso.fit,newx=x.testm) 
RMSE(lasso.pred,te$SalePrice)
```



```{r,cache=TRUE}
#Elastic Net

cl <- makePSOCKcluster(5)
registerDoParallel(cl)

train_control <- trainControl(method = "repeatedcv",
                              number = 5,
                              repeats = 3,
                              search = "random",
                              )

# Train the model
elastic.fit <- train(SalePrice ~ .,
                           data = tr,
                           method = "glmnet",
                           tuneLength = 25,
                           trControl = train_control,
                           metric = "RMSE")

stopCluster(cl)
```

```{r}
# Check RMSE:
elastic.pred<- predict(elastic.fit, te)
RMSE(elastic.pred,te$SalePrice)
```




```{r,cache=TRUE}
#pcr

cl <- makePSOCKcluster(5)
registerDoParallel(cl)

pcr.control <- trainControl(method = "repeatedcv",
                              number = 5,
                              repeats = 3,
                              search = "random",
                              )

# Train the model
pcr.fit <- train(SalePrice ~ .,
                           data = tr,
                           method = "pcr",
                           tuneLength = 25,
                           trControl = pcr.control,
                           metric = "RMSE")

stopCluster(cl)
```


```{r}
pcr.pred=predict(pcr.fit,te) 
RMSE(pcr.pred,te$SalePrice)
```
```{r,cache=TRUE}
#pcr

cl <- makePSOCKcluster(5)
registerDoParallel(cl)

pcr.control <- trainControl(method = "repeatedcv",
                              number = 5,
                              repeats = 3,
                              search = "random",
                              )

# Train the model
pcr.fit <- train(SalePrice ~ .,
                           data = full.t[train,],
                           method = "pcr",
                           tuneLength = 25,
                           trControl = pcr.control,
                           metric = "RMSE")

stopCluster(cl)
```

```{r}
pcr.pred2 <- predict(pcr.fit, full.t[test,-205])

final <- data.frame(Id = row.names(full.t[test,]), SalePrice = as.integer(round(exp(pcr.pred2) / 500) * 500))
write.csv(final, "/Users/apple/Desktop/submission.csv", row.names = F)
head(final$SalePrice)
```



```{r}
#PLS
cl <- makePSOCKcluster(5)
registerDoParallel(cl)

pls.control <- trainControl(method = "repeatedcv",
                              number = 5,
                              repeats = 3,
                              search = "random",
                              )

# Train the model
pls.fit <- train(SalePrice ~ .,
                           data = tr,
                           method = "pls",
                           tuneLength = 25,
                           trControl = pls.control,
                           metric = "RMSE")

stopCluster(cl)

```


```{r}
pls.pred=predict(pls.fit,te,ncomp=12) 
RMSE(pls.pred,te$SalePrice)
```



```{r,cache=TRUE}
#Random Forest:importance
set.seed(100)
cl <- makePSOCKcluster(5)
registerDoParallel(cl)

rf.control <- trainControl(method='repeatedcv', 
                           repeats=5,
                           number=3)

mtry <- sqrt(205)
tunegrid <- expand.grid(.mtry=1:mtry)

rf.fit <- train(SalePrice~., 
                      data=tr, 
                      method='rf', 
                      metric='RMSE', 
                      tuneGrid=tunegrid, 
                      trControl=rf.control)

stopCluster(cl)
```




```{r}
print(rf.fit)
```
```{r}
rf.pred=predict(rf.fit,te)
RMSE(rf.pred,te$SalePrice)
```


```{r}
#Visualizing feature importance
importance <- varImp(rf.fit)$importance
varImportance <- data.frame(Variables = row.names(importance), 
                            Importance = round(importance$Overall,2))
varImportance
```

```{r}
# Create a rank variable based on importance
rankImportance <- varImportance %>%
  mutate(Rank = paste0('#',dense_rank(desc(Importance))))

# Use ggplot2 to visualize the relative importance of variables
ggplot(rankImportance, aes(x = reorder(Variables, Importance), 
    y = Importance, fill = Importance)) +
  geom_bar(stat='identity') + 
  geom_text(aes(x = Variables, y = 0.5, label = Rank),
    hjust=0, vjust=0.55, size = 4, colour = 'red') +
  labs(x = 'Variables') +
  coord_flip()
```




```{r}
#XGBoost

#Create matrices from the data frames
trainData<- as.matrix(tr, rownames.force=NA)
testData<- as.matrix(te, rownames.force=NA)

#Turn the matrices into sparse matrices
xgb.tr <- as(trainData, "sparseMatrix")
xgb.te <- as(testData, "sparseMatrix")


#Cross Validate the model

trainD <- xgb.DMatrix(data = xgb.tr[,-205], label = xgb.tr[,"SalePrice"]) #Convert to xgb.DMatrix format

#Cross validate the model
cv.sparse <- xgb.cv(data = trainD,
                    nrounds = 600,
                    min_child_weight = 0,
                    max_depth = 10,
                    eta = 0.02,
                    subsample = .7,
                    colsample_bytree = .7,
                    booster = "gbtree",
                    eval_metric = "rmse",
                    verbose = TRUE,
                    print_every_n = 50,
                    nfold = 4,
                    nthread = 2,
                    objective="reg:linear")
```


```{r}
#Train the model

#Choose the parameters for the model
param <- list(colsample_bytree = .7,
             subsample = .7,
             booster = "gbtree",
             max_depth = 10,
             eta = 0.02,
             eval_metric = "rmse",
             objective="reg:linear")

#Train the model using those parameters
bstSparse <-
  xgb.train(params = param,
            data = trainD,
            nrounds = 600,
            watchlist = list(train = trainD),
            verbose = TRUE,
            print_every_n = 50,
            nthread = 2)
```
```{r}
testD <- xgb.DMatrix(data = xgb.te[,-205])

xgb.pred <- predict(bstSparse, testD) 

RMSE(xgb.pred,te$SalePrice)
```
```{r}
#Retrain on the full sample
rm(bstSparse)
#Create matrices from the data frames
retrainData<- as.matrix(full.t[train,], rownames.force=NA)

#Turn the matrices into sparse matrices
retrain <- as(retrainData, "sparseMatrix")

param <- list(colsample_bytree = .7,
             subsample = .7,
             booster = "gbtree",
             max_depth = 10,
             eta = 0.02,
             eval_metric = "rmse",
             objective="reg:linear")

retrainD <- xgb.DMatrix(data = retrain[,-205], label = retrain[,"SalePrice"])

#retrain the model using those parameters
bstSparse <-
 xgb.train(params = param,
           data = retrainD,
           nrounds = 600,
           watchlist = list(train = retrainD),
           verbose = TRUE,
           print_every_n = 50,
           nthread = 2)
```

```{r}

#Create matrices from the data frames
final.ma<- as.matrix(full.t[test,-205], rownames.force=NA)

#Turn the matrices into sparse matrices
final.sm <- as(final.ma, "sparseMatrix")

```



```{r}

xgb.pred2 <- predict(bstSparse, final.sm)

final <- data.frame(Id = row.names(full.t[test,]), SalePrice = as.integer(round(exp(xgb.pred2) / 500) * 500))

```

```{r}
write.csv(final, "/Users/apple/Desktop/submission.csv", row.names = F)
head(final$SalePrice)
```

