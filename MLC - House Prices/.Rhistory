preProc = "zv",
trControl = gbm.control,
tuneGrid = gbm.grid)
stopCluster(cl)
gbm.fit
plot(gbm.fit)
gbm.pred=predict(gbm.fit,te)
RMSE(gbm.pred,te$SalePrice)
#GBM
set.seed(100)
cl <- makePSOCKcluster(5)
registerDoParallel(cl)
gbm.control <- trainControl(method = "repeatedcv",
number = 5,
repeats = 3)
gbm.grid <-  expand.grid(interaction.depth = c(9,11,13),
n.trees = 2000,
shrinkage = 0.01,
n.minobsinnode = c(2,3,4))
gbm.fit2 <- train(SalePrice ~ ., data = full[train,],
method = "gbm",
preProc = "zv",
trControl = gbm.control,
tuneGrid = gbm.grid)
stopCluster(cl)
plot(gbm.fit2, metric = "RMSE", plotType = "level",scales = list(x = list(rot = 90)))
gbm.pred2 <- predict(gbm.fit2, testing[,-69])
gbm.pred2 <- predict(gbm.fit2, full[test,-69])
final <- data.frame(Id = row.names(full[test,]), SalePrice = as.integer(round(exp(gbm.pred2) / 500) * 500))
write.csv(final, "/Users/apple/Desktop/submission.csv", row.names = F)
head(final$SalePrice)
#XGBoost
#Number of trees:100-1000,Tree depth:4-10, Learning rate.
#Row sampling:0.5-1.0, column sampling:0.3-0.5, min leaf weight:1/sqrt(event rate), min split gain:0
#early stopping
#Create matrices from the data frames
trainData<- as.matrix(tr, rownames.force=NA)
testData<- as.matrix(te, rownames.force=NA)
#Turn the matrices into sparse matrices
xgb.tr <- as(trainData, "sparseMatrix")
xgb.te <- as(testData, "sparseMatrix")
#Train and test set split
z <- full.t[train,]
partition <- createDataPartition(y=z$SalePrice,
p=.5,
list=F)
tr <- z[partition,]
te <- z[-partition,]
#XGBoost
#Number of trees:100-1000,Tree depth:4-10, Learning rate.
#Row sampling:0.5-1.0, column sampling:0.3-0.5, min leaf weight:1/sqrt(event rate), min split gain:0
#early stopping
#Create matrices from the data frames
trainData<- as.matrix(tr, rownames.force=NA)
testData<- as.matrix(te, rownames.force=NA)
#Turn the matrices into sparse matrices
xgb.tr <- as(trainData, "sparseMatrix")
xgb.te <- as(testData, "sparseMatrix")
xgb.tr
xgb.tr[-205]
xgb.tr[,-205]
trainD <- xgb.DMatrix(data = xgb.tr[,-205], label = xgb.tr[,"SalePrice"]) #Convert to xgb.DMatrix format
#Train the model
#Choose the parameters for the model
param <- list(colsample_bytree = .7,
subsample = .7,
booster = "gbtree",
max_depth = 10,
min_child_weight = 0,
eta = 0.02,
eval_metric = "rmse",
objective="reg:linear",
early_stopping_rounds = 10)
#Train the model using those parameters
bstSparse <-
xgb.train(params = param,
data = trainD,
nrounds = 600,
watchlist = list(train = trainD),
verbose = TRUE,
print_every_n = 50,
nthread = 2)
testD <- xgb.DMatrix(data = xgb.te[,-205])
xgb.pred <- predict(bstSparse, testD)
RMSE(xgb.pred,te$SalePrice)
#Train the model
#Choose the parameters for the model
param <- list(colsample_bytree = .7,
subsample = .7,
booster = "gbtree",
max_depth = 10,
min_child_weight = 0,
eta = 0.02,
eval_metric = "rmse",
objective="reg:linear",
early_stopping_rounds = 20)
#Train the model using those parameters
bstSparse <-
xgb.train(params = param,
data = trainD,
nrounds = 600,
watchlist = list(train = trainD),
verbose = TRUE,
print_every_n = 50,
nthread = 2)
testD <- xgb.DMatrix(data = xgb.te[,-205])
xgb.pred <- predict(bstSparse, testD)
RMSE(xgb.pred,te$SalePrice)
#Train the model
#Choose the parameters for the model
param <- list(colsample_bytree = .7,
subsample = .7,
booster = "gbtree",
max_depth = 10,
min_child_weight = 0,
eta = 0.02,
eval_metric = "rmse",
objective="reg:linear",
early_stopping_rounds = 10)
#Train the model using those parameters
bstSparse <-
xgb.train(params = param,
data = trainD,
nrounds = 600,
watchlist = list(train = trainD),
verbose = TRUE,
print_every_n = 50,
nthread = 2)
testD <- xgb.DMatrix(data = xgb.te[,-205])
xgb.pred <- predict(bstSparse, testD)
RMSE(xgb.pred,te$SalePrice)
#Train and test set split
z <- full.s[train,]
partition <- createDataPartition(y=z$SalePrice,
p=.5,
list=F)
tr <- z[partition,]
te <- z[-partition,]
#Number of trees:100-1000,Tree depth:4-10, Learning rate.
#Row sampling:0.5-1.0, column sampling:0.3-0.5, min leaf weight:1/sqrt(event rate), min split gain:0
#early stopping
#Create matrices from the data frames
trainData<- as.matrix(tr, rownames.force=NA)
testData<- as.matrix(te, rownames.force=NA)
#Turn the matrices into sparse matrices
xgb.tr <- as(trainData, "sparseMatrix")
xgb.te <- as(testData, "sparseMatrix")
trainD <- xgb.DMatrix(data = xgb.tr[,-115], label = xgb.tr[,"SalePrice"]) #Convert to xgb.DMatrix format
#Train the model
#Choose the parameters for the model
param <- list(colsample_bytree = .7,
subsample = .7,
booster = "gbtree",
max_depth = 10,
min_child_weight = 0,
eta = 0.02,
eval_metric = "rmse",
objective="reg:linear",
early_stopping_rounds = 10)
#Train the model using those parameters
bstSparse <-
xgb.train(params = param,
data = trainD,
nrounds = 600,
watchlist = list(train = trainD),
verbose = TRUE,
print_every_n = 50,
nthread = 2)
testD <- xgb.DMatrix(data = xgb.te[,-115])
xgb.pred <- predict(bstSparse, testD)
RMSE(xgb.pred,te$SalePrice)
#Train and test set split
z <- full.t[train,]
partition <- createDataPartition(y=z$SalePrice,
p=.5,
list=F)
tr <- z[partition,]
te <- z[-partition,]
#Number of trees:100-1000,Tree depth:4-10, Learning rate.
#Row sampling:0.5-1.0, column sampling:0.3-0.5, min leaf weight:1/sqrt(event rate), min split gain:0
#early stopping
#Create matrices from the data frames
trainData<- as.matrix(tr, rownames.force=NA)
testData<- as.matrix(te, rownames.force=NA)
#Turn the matrices into sparse matrices
xgb.tr <- as(trainData, "sparseMatrix")
xgb.te <- as(testData, "sparseMatrix")
trainD <- xgb.DMatrix(data = xgb.tr[,-205], label = xgb.tr[,"SalePrice"]) #Convert to xgb.DMatrix format
#Train the model
#Choose the parameters for the model
param <- list(colsample_bytree = .7,
subsample = .7,
booster = "gbtree",
max_depth = 10,
min_child_weight = 0,
eta = 0.02,
eval_metric = "rmse",
objective="reg:linear",
early_stopping_rounds = 10)
#Train the model using those parameters
bstSparse <-
xgb.train(params = param,
data = trainD,
nrounds = 600,
watchlist = list(train = trainD),
verbose = TRUE,
print_every_n = 50,
nthread = 2)
testD <- xgb.DMatrix(data = xgb.te[,-205])
xgb.pred <- predict(bstSparse, testD)
RMSE(xgb.pred,te$SalePrice)
#Train the model
#Choose the parameters for the model
param <- list(colsample_bytree = .7,
subsample = .7,
booster = "gbtree",
max_depth = 10,
min_child_weight = 0,
eta = 0.02,
eval_metric = "rmse",
objective="reg:linear",
early_stopping_rounds = 10)
#Train the model using those parameters
bstSparse <-
xgb.train(params = param,
data = trainD,
nrounds = 800,
watchlist = list(train = trainD),
verbose = TRUE,
print_every_n = 50,
nthread = 2)
testD <- xgb.DMatrix(data = xgb.te[,-205])
xgb.pred <- predict(bstSparse, testD)
RMSE(xgb.pred,te$SalePrice)
#Train the model
#Choose the parameters for the model
param <- list(colsample_bytree = .7,
subsample = .7,
booster = "gbtree",
max_depth = 10,
min_child_weight = 0,
eta = 0.02,
eval_metric = "rmse",
objective="reg:linear",
early_stopping_rounds = 10)
#Train the model using those parameters
bstSparse <-
xgb.train(params = param,
data = trainD,
nrounds = 1000,
watchlist = list(train = trainD),
verbose = TRUE,
print_every_n = 50,
nthread = 2)
testD <- xgb.DMatrix(data = xgb.te[,-205])
xgb.pred <- predict(bstSparse, testD)
RMSE(xgb.pred,te$SalePrice)
#Train the model
#Choose the parameters for the model
param <- list(colsample_bytree = .7,
subsample = .7,
booster = "gbtree",
max_depth = 10,
min_child_weight = 0,
eta = 0.02,
eval_metric = "rmse",
objective="reg:linear",
early_stopping_rounds = 10)
#Train the model using those parameters
bstSparse <-
xgb.train(params = param,
data = trainD,
nrounds = 600,
watchlist = list(train = trainD),
verbose = TRUE,
print_every_n = 50,
nthread = 2)
testD <- xgb.DMatrix(data = xgb.te[,-205])
xgb.pred <- predict(bstSparse, testD)
RMSE(xgb.pred,te$SalePrice)
#Train the model
#Choose the parameters for the model
param <- list(colsample_bytree = .7,
subsample = .7,
booster = "gbtree",
max_depth = 10,
min_child_weight = 0,
eta = 0.02,
eval_metric = "rmse",
objective="reg:linear",
early_stopping_rounds = 10)
#Train the model using those parameters
bstSparse <-
xgb.train(params = param,
data = trainD,
nrounds = 500,
watchlist = list(train = trainD),
verbose = TRUE,
print_every_n = 50,
nthread = 2)
testD <- xgb.DMatrix(data = xgb.te[,-205])
xgb.pred <- predict(bstSparse, testD)
RMSE(xgb.pred,te$SalePrice)
#Train the model
#Choose the parameters for the model
param <- list(colsample_bytree = .7,
subsample = .7,
booster = "gbtree",
max_depth = 10,
min_child_weight = 0,
eta = 0.02,
eval_metric = "rmse",
objective="reg:linear",
early_stopping_rounds = 10)
#Train the model using those parameters
bstSparse <-
xgb.train(params = param,
data = trainD,
nrounds = 600,
watchlist = list(train = trainD),
verbose = TRUE,
print_every_n = 50,
nthread = 2)
testD <- xgb.DMatrix(data = xgb.te[,-205])
xgb.pred <- predict(bstSparse, testD)
RMSE(xgb.pred,te$SalePrice)
#Retrain on the full sample
#Create matrices from the data frames
retrainData<- as.matrix(full.t[train,], rownames.force=NA)
#Turn the matrices into sparse matrices
retrain <- as(retrainData, "sparseMatrix")
retrainD <- xgb.DMatrix(data = retrain[,-205], label = retrain[,"SalePrice"])
set.seed(100)
cl <- makePSOCKcluster(5)
registerDoParallel(cl)
xgb.control <- trainControl(method = "cv",
number = 5)
xgb.grid <- expand.grid(nrounds = c(600,800,1000),
max_depth = c(6,8,10),
eta = 0.01,
gamma = c(0.0),#0.2, 1),
colsample_bytree = c(0.8),#0.5, 1),
min_child_weight=c(1),
subsample=c(0.8)
)
xgb.fit <-train(SalePrice ~.,
data=retrain,
method="xgbTree",
metric = "RMSE",
trControl=xgb.ctrl,
tuneGrid=xgb.grid
)
set.seed(100)
cl <- makePSOCKcluster(5)
registerDoParallel(cl)
xgb.control <- trainControl(method = "cv",
number = 5)
xgb.grid <- expand.grid(nrounds = c(600,800,1000),
max_depth = c(6,8,10),
eta = 0.01,
gamma = c(0.0),#0.2, 1),
colsample_bytree = c(0.8),#0.5, 1),
min_child_weight=c(1),
subsample=c(0.8)
)
xgb.fit <-train(SalePrice ~.,
data=full.t[train,],
method="xgbTree",
metric = "RMSE",
trControl=xgb.ctrl,
tuneGrid=xgb.grid
)
set.seed(100)
cl <- makePSOCKcluster(5)
registerDoParallel(cl)
xgb.ctrl <- trainControl(method = "cv",
number = 5)
xgb.grid <- expand.grid(nrounds = c(600,800,1000),
max_depth = c(6,8,10),
eta = 0.01,
gamma = c(0.0),#0.2, 1),
colsample_bytree = c(0.8),#0.5, 1),
min_child_weight=c(1),
subsample=c(0.8)
)
xgb.fit <-train(SalePrice ~.,
data=full.t[train,],
method="xgbTree",
metric = "RMSE",
trControl=xgb.ctrl,
tuneGrid=xgb.grid
)
stopCluster(cl)
plot(xgb.fit)
set.seed(100)
cl <- makePSOCKcluster(5)
registerDoParallel(cl)
xgb.ctrl <- trainControl(method = "cv",
number = 5)
xgb.grid <- expand.grid(nrounds = c(1000,1500),
max_depth = c(4,6,8,10),
eta = 0.01,
gamma = c(0.0),#0.2, 1),
colsample_bytree = c(0.8),#0.5, 1),
min_child_weight=c(1),
subsample=c(0.8)
)
xgb.fit <-train(SalePrice ~.,
data=full.t[train,],
method="xgbTree",
metric = "RMSE",
trControl=xgb.ctrl,
tuneGrid=xgb.grid
)
stopCluster(cl)
plot(xgb.fit)
set.seed(100)
cl <- makePSOCKcluster(5)
registerDoParallel(cl)
xgb.ctrl <- trainControl(method = "cv",
number = 5)
xgb.grid <- expand.grid(nrounds = c(1500,2000),
max_depth = c(2,3,4),
eta = 0.01,
gamma = c(0.0),#0.2, 1),
colsample_bytree = c(0.8),#0.5, 1),
min_child_weight=c(1),
subsample=c(0.8)
)
xgb.fit <-train(SalePrice ~.,
data=full.t[train,],
method="xgbTree",
metric = "RMSE",
trControl=xgb.ctrl,
tuneGrid=xgb.grid
)
stopCluster(cl)
#Row sampling:0.5-1.0, column sampling:0.3-0.5, min leaf weight:1/sqrt(event rate), min split gain:0
plot(xgb.fit)
set.seed(100)
cl <- makePSOCKcluster(5)
registerDoParallel(cl)
xgb.ctrl <- trainControl(method = "cv",
number = 5)
xgb.grid <- expand.grid(nrounds = 2000,
max_depth = c(3,4),
eta = 0.01,
gamma = c(0.0, 0.2, 1),
colsample_bytree = c(0.8),#0.5, 1),
min_child_weight=c(1,10),
subsample=c(0.8)
)
xgb.fit <-train(SalePrice ~.,
data=full.t[train,],
method="xgbTree",
metric = "RMSE",
trControl=xgb.ctrl,
tuneGrid=xgb.grid
)
stopCluster(cl)
#Row sampling:0.5-1.0, column sampling:0.3-0.5, min leaf weight:1/sqrt(event rate), min split gain:0
plot(xgb.fit)
set.seed(100)
cl <- makePSOCKcluster(5)
registerDoParallel(cl)
xgb.ctrl <- trainControl(method = "cv",
number = 5)
xgb.grid <- expand.grid(nrounds = 2000,
max_depth = c(4),
eta = 0.01,
gamma = c(0.0),
colsample_bytree = c(0.7,0.5,0.3),
min_child_weight=c(10),
subsample=c(0.7,0.5,0.3)
)
xgb.fit <-train(SalePrice ~.,
data=full.t[train,],
method="xgbTree",
metric = "RMSE",
trControl=xgb.ctrl,
tuneGrid=xgb.grid
)
stopCluster(cl)
#Row sampling:0.5-1.0, column sampling:0.3-0.5, min leaf weight:1/sqrt(event rate), min split gain:0
plot(xgb.fit)
set.seed(100)
cl <- makePSOCKcluster(5)
registerDoParallel(cl)
xgb.ctrl <- trainControl(method = "cv",
number = 5)
xgb.grid <- expand.grid(nrounds = 2000,
max_depth = c(3,4,5),
eta = 0.01,
gamma = c(0.0),
colsample_bytree = c(0.7,0.5),
min_child_weight=c(10),
subsample=c(0.9,0.7)
)
xgb.fit <-train(SalePrice ~.,
data=full.t[train,],
method="xgbTree",
metric = "RMSE",
trControl=xgb.ctrl,
tuneGrid=xgb.grid
)
stopCluster(cl)
plot(xgb.fit)
xgb.pred <- predict(xgb.fit, full.t[test,-205])
final <- data.frame(Id = row.names(full.t[test,]), SalePrice = as.integer(round(exp(xgb.pred) / 500) * 500))
#Output the result:
write.csv(final, "/Users/apple/Desktop/submission.csv", row.names = F)
head(final$SalePrice)
